ve#summary How to use HarvestMan events API to write custom crawlers

==HarvestMan events API==
HarvestMan provides a very well-defined events API which can be used by developers to write custom crawlers suited for a specific crawling/data mining task. 

===Events===
Events are implemented using a callback mechanism. At different times during the program execution, HarvestMan raises events with specific names. These events can be hooked into custom functions by subscribing to the events and defining functions which process the state supplied along with the event. 

In short the event API allows the developer to hook into the program flow and modify the program behavior by writing custom functions.

==Illustration==
Let us say that you want to write a custom crawler which saves only images which are larger than 4K to the disk (a practical example of this would be a crawler which ignores thumbnail images, since thumbnails are typically  of size 2K-4K). This is how you would do this by subscribing to the *_save_url_data_* event.

First you need to define a custom crawler class over-riding the _HarvestMan_ class.

{{{
from harvestman.apps.spider import HarvestMan
from harvestman.lib.common.macros import *

class MyCustomCrawler(HarvestMan):
    """ A custom crawler """

    size_threshold = 4096

    def save_this_url(self, event, *args, **kwargs):
        """ Custom callback function which modifies behaviour
            of saving URLs to disk """

        # Get the url object
        url = event.url
        # If not image, save always
        if not url.is_image():
            return True
        else:
            # If image, check for content-length > 4K
            size = url.clength
            return (size>self.size_threshold)

# Set up the custom crawler
if __name__ == "__main__":
    crawler = MyCustomCrawler()
    crawler.initialize()
    # Get the configuration object
    config = crawler.get_config()
    # Register for 'save_url_data' event which will be called
    # back just before a URL is saved to disk
    crawler.register('save_url_data', crawler.save_this_url)
    # Run
    crawler.main()
}}}

You can run the program as if you would run HarvestMan. For example if you save this code to a file named _customcrawler.py_ then you would run it as,

{{{
$ python customcrawler.py [URL]
}}}

Here is a sample crawl of a site containing images.
{{{
$ python customcrawler.py http://www.tcm.phy.cam.ac.uk/~pdh1001/Photo_Album/Kilimanjaro/pic0001.html
/usr/local/lib/python2.6/dist-packages/HarvestMan-2.0.4betadev_r210-py2.6.egg/harvestman/lib/crawler.py:53: DeprecationWarning: the sha module is deprecated; use the hashlib module instead
  import sha
/usr/local/lib/python2.6/dist-packages/HarvestMan-2.0.4betadev_r210-py2.6.egg/harvestman/lib/urlparser.py:50: DeprecationWarning: the md5 module is deprecated; use hashlib instead
  import md5
Loading user configuration... 
Starting HarvestMan 2.0 beta 5... 
Copyright (C) 2004, Anand B Pillai 
  
[2010-02-10 19:21:51,052] *** Log Started ***
 
[2010-02-10 19:21:51,052] Starting project www.tcm.phy.cam.ac.uk ...
[2010-02-10 19:21:51,052] Writing Project Files... 
[2010-02-10 19:21:51,191] Starting download of url http://www.tcm.phy.cam.ac.uk/~pdh1001/Photo_Album/Kilimanjaro/pic0001.html ...
[2010-02-10 19:21:51,250] Reading Project Cache... 
[2010-02-10 19:21:51,253] Project cache not found 
[2010-02-10 19:21:51,256] Downloading http://www.tcm.phy.cam.ac.uk/~pdh1001/Photo_Album/Kilimanjaro/pic0001.html
[2010-02-10 19:21:52,211] Saved /home/anand/work/harvestman/HarvestMan-lite/harvestman/apps/samples/www.tcm.phy.cam.ac.uk/www.tcm.phy.cam.ac.uk/~pdh1001/Photo_Album/Kilimanjaro/pic0001.html
[2010-02-10 19:21:52,299] Fetching links http://www.tcm.phy.cam.ac.uk/~pdh1001/Photo_Album/Kilimanjaro/pic0001.html
[2010-02-10 19:21:52,730] Downloading http://www.tcm.phy.cam.ac.uk/~pdh1001/Photo_Album/Kilimanjaro/index.html
[2010-02-10 19:21:52,731] Downloading http://www.tcm.phy.cam.ac.uk/~pdh1001/Photo_Album/Kilimanjaro/pic0002.html
...
}}}

==Diving deep==
Let us dissect the custom crawler application we have built to understand the
events API.

Here are the steps involved.

 * Create a custom crawler class inheriting the _HarvestMan_ class.
 * Create a custom function which hooks into a specific event.
 * In the {{{__main__}}} section, configure the crawler to subscribe to the event by using the _register_ method.

Let us take a deeper look at the function _save_this_url_.

{{{
    def save_this_url(self, event, *args, **kwargs):
        """ Custom callback function which modifies behaviour
            of saving URLs to disk """

        # Get the url object
        url = event.url
        # If not image, save always
        if not url.is_image():
            return True
        else:
            # If image, check for content-length > 4K
            size = url.clength
            return (size>self.size_threshold)
}}}

The main object here of interest is the *_event_* object. This object contains
all the state the programmer needs to write the custom behavior. The _event_
object is of type *_Event_*, a class defined in the module _harvestman.lib.event_.

The _Event_ class is defined as follows.
{{{
class Event(object):
    """ Event class for HarvestMan """

    def __init__(self):
        self.name = ''
        self.config = objects.config
        self.url = None
        self.document = None
}}}

The attributes of the class are namely, _name_, _config_, _url_ and _document_. Of these the attributes of primary interest to the developer are _url_ and _document_.

===The _url_ attribute==
The _url_ attribute contains the current URL object which is being processed. The URL object is of type _HarvestManUrl_ (in module _harvestman.lib.urlparser_). It keeps
all the state of the current URL under processing. 

===The _document_ attribute===
The _document_ attribute holds information on the current web-page being crawled.
The _document_ object is of type _HarvestManDocument_ (module _harvestman.lib.document_). This object holds information on the URL as a document, i.e its content, etag, last modified time etc. The document object is useful for URLs which represent web-pages or documents such as PDF etc.

===The _config_ attribute===
This attribute binds to the global configuration object. Instead of having to call _objects.config_ everytime, you can get global configuration in the event handler by accessing the _event.config_ attribute.

===The _name_ attribute===
This will contain the name of the event. For example in the above code, this would be _save_url_data_.

===How to use attributes of _event_ object===
For most events, the _url_ attribute it present and is required to do any meaningful processing. The _document_ attribute is present only for the events which are dealing with a web-page with parseable content. For some events which are related to program 
stages (such as start/end of a project), both these attributes wont be present, i.e they will be _None_.

===Additional arguments===
Additional arguments could be passed to the event handler by specific events. Positional arguments will appear as the _*args_ and keyword arguments as the _**kwargs_ variables respectively. For example the _before_tag_parse_ event passes in the current HTML tag and its attributes using positional arguments.

==Table of Events==
The following table lists all the current events raised by HarvestMan and the attributes that are filled in for each event, and the points in program flow when the events are raised.

|| *Event* || *Raised at* || *Attributes* || *Module* || *Positional Arguments* || *Comments* ||
|| before_start_project || Before starting a project || url || harvestman.apps.spider || None || The url is the starting URL ||



